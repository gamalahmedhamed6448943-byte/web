import streamlit as st
import os
import sys
import subprocess
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from gtts import gTTS
from moviepy.editor import *
from moviepy.config import change_settings
from moviepy.video.fx.all import crop, resize
from PIL import Image, ImageFilter
import numpy as np
import textwrap
from rake_nltk import Rake
import nltk
import uuid
import shutil
import random
import PIL.Image

# Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ ÙŠØ¹ÙŠØ¯ ØªØ¹Ø±ÙŠÙ ANTIALIAS Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙÙ‚ÙˆØ¯Ø© Ù„ÙŠØ¹Ù…Ù„ MoviePy
if not hasattr(PIL.Image, 'ANTIALIAS'):
    PIL.Image.ANTIALIAS = PIL.Image.LANCZOS

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØªØ«Ø¨ÙŠØª NLTK ---
try:
    if os.name == 'posix':
        change_settings({"IMAGEMAGICK_BINARY": "/usr/bin/convert"})
except:
    pass

@st.cache_resource
def download_nltk_resources():
    nltk.download('stopwords', quiet=True)
    nltk.download('punkt', quiet=True)
    nltk.download('punkt_tab', quiet=True)

download_nltk_resources()

# ==============================================================================
# 1. ÙƒÙˆØ¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ± (ÙƒÙ…Ø§ Ù‡Ùˆ)
# ==============================================================================

def get_best_image_url(img_tag, base_url):
    srcset = img_tag.get('srcset') or img_tag.get('data-srcset')
    if srcset:
        try:
            candidates = []
            for entry in srcset.split(','):
                parts = entry.strip().split()
                if len(parts) >= 1:
                    url = parts[0]
                    width = 0
                    if len(parts) > 1 and 'w' in parts[1]:
                        width = int(parts[1].replace('w', ''))
                    candidates.append((width, url))
            if candidates:
                best_candidate = sorted(candidates, key=lambda x: x[0], reverse=True)[0]
                return urljoin(base_url, best_candidate[1])
        except:
            pass
    data_src = img_tag.get('data-src') or img_tag.get('data-original')
    if data_src:
        return urljoin(base_url, data_src)
    src = img_tag.get('src')
    if src:
        return urljoin(base_url, src)
    return None

def check_image_size_is_valid(url):
    try:
        response = requests.head(url, timeout=5, allow_redirects=True)
        if response.status_code != 200:
            response = requests.get(url, stream=True, timeout=5)
        content_length = response.headers.get('Content-Length')
        if content_length:
            size_kb = int(content_length) / 1024
            if size_kb < 6:
                 return False
        return True
    except:
        return False

def advanced_extract_images(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'}
    extracted_images = []
    try:
        st.info(f"ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ± Ù…Ù†: {url} ...")
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        for tag in soup(['script', 'style', 'header', 'footer', 'nav', 'aside', 'noscript', 'iframe', 'svg']):
            tag.decompose()
        target_area = soup.find('article') or soup.find('main') or soup.find(role='main') or soup.find(id=lambda x: x and 'content' in x)
        if not target_area: target_area = soup
        img_tags = target_area.find_all('img')
        seen_urls = set()

        for i, img in enumerate(img_tags):
            full_url = get_best_image_url(img, url)
            if not full_url: continue
            full_url = full_url.split('?')[0]
            ext_check = full_url.lower()
            if ext_check.endswith('.svg') or ext_check.endswith('.gif') or ext_check.endswith('.ico'): continue
            if 'data:image' in ext_check and len(ext_check) < 1000: continue
            bad_words = ['logo', 'icon', 'avatar', 'profile', 'sprite', 'pixel', 'blank', 'transparent']
            if any(w in ext_check for w in bad_words): continue
            if full_url in seen_urls: continue
            if check_image_size_is_valid(full_url):
                extracted_images.append(full_url)
                seen_urls.add(full_url)

        if not extracted_images:
            st.warning("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØµÙˆØ± Ù…Ø­ØªÙˆÙ‰ Ø­Ù‚ÙŠÙ‚ÙŠØ©.")
        else:
            st.success(f"ğŸ‰ ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(extracted_images)} ØµÙˆØ±Ø©.")
        return extracted_images
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØµÙˆØ±: {e}")
        return []

# ==============================================================================
# 2. ÙƒÙˆØ¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ (ÙƒÙ…Ø§ Ù‡Ùˆ)
# ==============================================================================

def extract_text_content_data(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'}
    try:
        st.info(f"ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ù†Øµ Ù…Ù†: {url} ...")
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        article_title = "No Title Found"
        h1 = soup.find('h1')
        if h1:
            article_title = h1.get_text(strip=True)
        else:
            title_tag = soup.find('title')
            if title_tag:
                article_title = title_tag.get_text(strip=True).split('-')[0].strip()

        useless_tags = ['script', 'style', 'header', 'footer', 'nav', 'aside', 'noscript', 'iframe', 'svg', 'form', 'button', 'figcaption', 'figure', 'video']
        for tag in list(soup(useless_tags)): tag.decompose()
        
        bad_classes = ['ad', 'advert', 'social', 'share', 'cookie', 'popup', 'promo', 'related-content', 'outbrain', 'taboola']
        for tag in list(soup.find_all(True)):
            if tag is None: continue
            try:
                classes = tag.get('class', [])
                if classes:
                    class_str = " ".join(classes).lower()
                    if any(bad in class_str for bad in bad_classes):
                        tag.decompose()
            except: pass

        target_area = soup.find('article') or soup.find('div', class_=lambda x: x and 'article' in x.lower() and 'body' in x.lower()) or soup.find('div', class_=lambda x: x and 'content' in x.lower()) or soup.find('main')
        if not target_area: target_area = soup
        
        paragraphs = []
        raw_text_list = []
        elements = target_area.find_all(['p', 'h2', 'h3'])
        
        for element in elements:
            text = element.get_text(strip=True)
            if len(text) < 20 and element.name == 'p': continue
            forbidden_phrases = ["Read more", "Follow us", "Copyright", "All rights reserved", "Image source", "Sign up", "Click here", "Ad Feedback", "Story highlights", "CNN", "BBC"]
            if any(phrase.lower() in text.lower() for phrase in forbidden_phrases): continue
            
            raw_text_list.append(text)
            if element.name in ['h2', 'h3']:
                paragraphs.append(f"<h3>{text}</h3>")
            else:
                paragraphs.append(f"<p>{text}</p>")

        full_clean_text = ". ".join(raw_text_list)
        st.success(f"ğŸ‰ ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(paragraphs)} ÙÙ‚Ø±Ø© Ù†ØµÙŠØ©.")
        return article_title, full_clean_text, raw_text_list
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ: {e}")
        return None, None, None

# ==============================================================================
# 3. Ù…Ø­Ø±Ùƒ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ© (Ù…Ø¹Ø¯Ù„ Ù„Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„ØªØ¹Ø¯Ø¯ÙŠØ©)
# ==============================================================================

def create_moving_backdrop_clip(img_path, duration, screen_size=(1280, 720), zoom_direction='in', speed_factor=0.06):
    """
    ØªÙ… Ø§Ù„ØªØ¹Ø¯ÙŠÙ„:
    1. zoom_direction: Ù„ØªØºÙŠÙŠØ± Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø­Ø±ÙƒØ© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ§Ù‹.
    2. speed_factor: Ø²Ø§Ø¯Øª Ø§Ù„Ø³Ø±Ø¹Ø© Ù…Ù† 0.02 Ø¥Ù„Ù‰ 0.06 Ù„ØªÙƒÙˆÙ† 'fast and continuous'.
    """
    pil_img = Image.open(img_path)
    
    # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø®Ù„ÙÙŠØ© Ø§Ù„Ù…Ø¶Ø¨Ø¨Ø©
    bg_img = pil_img.resize((screen_size[0], screen_size[1]), Image.LANCZOS)
    bg_img = bg_img.filter(ImageFilter.GaussianBlur(radius=15))
    
    bg_clip = ImageClip(np.array(bg_img)).set_duration(duration)
    
    # Ø­Ø±ÙƒØ© Ø§Ù„Ø®Ù„ÙÙŠØ© (Ø³Ø±ÙŠØ¹Ø© ÙˆÙ…Ø³ØªÙ…Ø±Ø©)
    if zoom_direction == 'in':
        # ØªÙƒØ¨ÙŠØ± Ø³Ø±ÙŠØ¹
        bg_clip = bg_clip.resize(lambda t: 1 + speed_factor * t)
    else:
        # ØªØµØºÙŠØ± Ø³Ø±ÙŠØ¹ (ÙŠØ¨Ø¯Ø£ Ù…ÙƒØ¨Ø±Ø§Ù‹ ÙˆÙŠØµØºØ±)
        bg_clip = bg_clip.resize(lambda t: (1 + speed_factor * duration) - speed_factor * t)
        
    bg_clip = bg_clip.set_position(('center', 'center'))
    
    # ØªØ­Ø¶ÙŠØ± Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©
    w, h = pil_img.size
    target_h = int(screen_size[1] * 0.9)
    ratio = w / h
    target_w = int(target_h * ratio)
    
    if target_w > screen_size[0] * 0.9:
        target_w = int(screen_size[0] * 0.9)
        target_h = int(target_w / ratio)
        
    fg_img = pil_img.resize((target_w, target_h), Image.LANCZOS)
    fg_clip = ImageClip(np.array(fg_img)).set_duration(duration)
    fg_clip = fg_clip.set_position(('center', 'center'))
    
    # Ø­Ø±ÙƒØ© Ø·ÙÙŠÙØ© Ù„Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ© Ø£ÙŠØ¶Ø§Ù‹ Ù„Ø¥Ø¶Ø§ÙØ© Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ©
    if zoom_direction == 'in':
         fg_clip = fg_clip.resize(lambda t: 1 + (speed_factor/2) * t)
    else:
         fg_clip = fg_clip.resize(lambda t: 1 + (speed_factor/2) * (duration - t))

    final_clip = CompositeVideoClip([bg_clip, fg_clip], size=screen_size).set_duration(duration)
    return final_clip

def generate_youtube_metadata(title, text_list, url):
    full_text = " ".join(text_list)
    r = Rake()
    r.extract_keywords_from_text(full_text)
    keywords = r.get_ranked_phrases()[:15]
    tags = [k for k in keywords if len(k) < 30]
    tags_str = ", ".join(tags)
    
    summary = "\n\n".join(text_list[:3])
    description = f""" {title} \n\n {summary} \n\n ğŸ‘‡ Read the full article here: {url} \n\n #News #{tags[0].replace(' ','')} #{tags[1].replace(' ','') if len(tags)>1 else 'Video'} """.strip()
    
    thumb_prompt = f"A high-quality YouTube thumbnail image representing '{title}'. Professional news style, high contrast, 4k resolution, featuring elements of {tags[0] if tags else 'news'}."
    
    return tags_str, description, thumb_prompt

def process_pipeline(url_input):
    if not url_input:
        st.warning("âŒ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø±Ø§Ø¨Ø·.")
        return

    # --- Ø¹Ø²Ù„ Ø§Ù„Ø¬Ù„Ø³Ø© (Multi-User Support) ---
    session_id = str(uuid.uuid4())
    session_dir = os.path.join(os.getcwd(), f"temp_{session_id}")
    os.makedirs(session_dir, exist_ok=True)
    
    # ØªØ­Ø¯ÙŠØ¯ Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…Ù„ÙØ§Øª Ø¯Ø§Ø®Ù„ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¬Ù„Ø³Ø©
    audio_file = os.path.join(session_dir, "generated_audio.mp3")
    output_filename = os.path.join(session_dir, "output_video.mp4")

    # ØªØ­Ø¯ÙŠØ¯ Ù…ØªØºÙŠØ±Ø§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù„Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ø­Ø§Ù„ÙŠ (Different Slide Transition every time)
    # Ù†Ø®ØªØ§Ø± Ø¹Ø´ÙˆØ§Ø¦ÙŠØ§Ù‹ Ø³Ø±Ø¹Ø© Ø§Ù„Ø­Ø±ÙƒØ© ÙˆØ§ØªØ¬Ø§Ù‡Ù‡Ø§ Ù„Ù‡Ø°Ø§ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ Ø£Ùˆ Ù„ÙƒÙ„ Ø´Ø±ÙŠØ­Ø©
    base_zoom_speed = random.uniform(0.04, 0.08) # Ø­Ø±ÙƒØ© Ø³Ø±ÙŠØ¹Ø© Ø¬Ø¯Ø§Ù‹
    transition_duration = random.uniform(0.5, 1.5) # Ù…Ø¯Ø© Ø§Ù†ØªÙ‚Ø§Ù„ Ù…ØªØºÙŠØ±Ø©

    # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
    title, full_text, text_list = extract_text_content_data(url_input)
    if not title or not full_text:
        st.error("âŒ ÙØ´Ù„ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ.")
        shutil.rmtree(session_dir, ignore_errors=True)
        return
        
    images_urls = advanced_extract_images(url_input)
    if not images_urls:
        st.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØµÙˆØ±ØŒ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø´Ø§Ø´Ø© Ø³ÙˆØ¯Ø§Ø¡ Ù…Ø¹ Ø§Ù„Ù†Øµ.")

    # 2. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØª (TTS)
    with st.spinner("ğŸ”Š Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØª..."):
        tts_text = f"{title}. {full_text}"
        if len(tts_text) > 5000:
            st.info("âš ï¸ Ø§Ù„Ù†Øµ Ø·ÙˆÙŠÙ„ Ø¬Ø¯Ø§Ù‹ØŒ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙˆÙ„ 5000 Ø­Ø±Ù Ù„Ù„ØµÙˆØª.")
            tts_text = tts_text[:5000]
            
        tts = gTTS(text=tts_text, lang='en')
        tts.save(audio_file)
        
        audio_clip = AudioFileClip(audio_file)
        audio_duration = audio_clip.duration
        st.success(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØª. Ø§Ù„Ù…Ø¯Ø©: {audio_duration:.2f} Ø«Ø§Ù†ÙŠØ©")

    # 3. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
    with st.spinner("ğŸ¬ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± ÙˆØ¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ (60 FPS)..."):
        if images_urls:
            downloaded_images = []
            for i, img_url in enumerate(images_urls):
                try:
                    img_data = requests.get(img_url).content
                    img_name = os.path.join(session_dir, f"temp_img_{i}.jpg")
                    with open(img_name, 'wb') as handler:
                        handler.write(img_data)
                    downloaded_images.append(img_name)
                except:
                    continue
            
            if not downloaded_images:
                st.error("âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±.")
                shutil.rmtree(session_dir, ignore_errors=True)
                return

            img_duration = audio_duration / len(downloaded_images)
            clips = []
            
            for i, img_path in enumerate(downloaded_images):
                # Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© ÙÙŠ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ù„ÙƒÙ„ Ø´Ø±ÙŠØ­Ø© Ù„ØªØºÙŠÙŠØ± Ø§Ù„Ù†Ù…Ø·
                direction = random.choice(['in', 'out'])
                
                clip = create_moving_backdrop_clip(
                    img_path, 
                    img_duration, 
                    zoom_direction=direction, 
                    speed_factor=base_zoom_speed
                )
                
                # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù†ØªÙ‚Ø§Ù„ (Transition)
                # Crossfade Ù‡Ùˆ Ø§Ù„Ø£ÙƒØ«Ø± Ø¯Ø¹Ù…Ø§Ù‹ ÙˆØ³Ø±Ø¹Ø© ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø®Ø§Ù…
                clip = clip.crossfadein(transition_duration)
                clips.append(clip)
            
            # Ø§Ù„Ø¯Ù…Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… compose Ù„Ø¶Ù…Ø§Ù† Ø¹Ù…Ù„ Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„Ø§Øª Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­
            final_video = concatenate_videoclips(clips, method="compose", padding=-transition_duration)
        else:
            color_clip = ColorClip(size=(1280, 720), color=(0,0,0), duration=audio_duration)
            txt_clip = TextClip(title, fontsize=70, color='white', size=(1000, None), method='caption')
            txt_clip = txt_clip.set_position('center').set_duration(audio_duration)
            final_video = CompositeVideoClip([color_clip, txt_clip])

        # 4. Ø¯Ù…Ø¬ Ø§Ù„ØµÙˆØª ÙˆØªØµØ¯ÙŠØ± Ø§Ù„ÙÙŠØ¯ÙŠÙˆ (Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ù‚ØµÙˆÙ‰)
        final_video = final_video.set_audio(audio_clip)
        
        st.text("âš™ï¸ Ø¬Ø§Ø±ÙŠ ØªØµØ¯ÙŠØ± Ø§Ù„ÙÙŠØ¯ÙŠÙˆ (Rendering) Ø¨Ø£Ù‚ØµÙ‰ Ø³Ø±Ø¹Ø© (Ultrafast, Multi-core, 60FPS)...")
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒÙ„ Ø§Ù„Ø£Ù†ÙˆÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø©
        cpu_count = os.cpu_count() or 2
        
        final_video.write_videofile(
            output_filename, 
            fps=60,                  # Ù…Ø·Ù„ÙˆØ¨: 60 Ø¥Ø·Ø§Ø±
            codec="libx264", 
            audio_codec="aac",
            preset="ultrafast",      # Ù…Ø·Ù„ÙˆØ¨: Ø£Ø³Ø±Ø¹ Ø¶ØºØ·
            threads=cpu_count        # Ù…Ø·Ù„ÙˆØ¨: Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒÙ„ Ø§Ù„Ø£Ù†ÙˆÙŠØ©
        )

        # 5. Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        st.success("âœ… COMPLETED SUCCESSFULLY")
        
        tags, desc, thumb = generate_youtube_metadata(title, text_list, url_input)
        
        st.subheader("ğŸ“‹ YOUTUBE DATA")
        st.text_area("Title", title)
        st.text_area("Description", desc)
        st.text_area("Tags", tags)
        st.info(f"**Thumbnail Prompt:** {thumb}")
        
        st.subheader("ğŸ¥ FINAL VIDEO")
        st.video(output_filename)
        
        with open(output_filename, "rb") as file:
            st.download_button(
                label="ğŸ“ Download Video",
                data=file,
                file_name=f"generated_video_{session_id[:8]}.mp4",
                mime="video/mp4"
            )

    # 6. ØªÙ†Ø¸ÙŠÙ Ù…Ù„ÙØ§Øª Ø§Ù„Ø¬Ù„Ø³Ø© (Cleanup)
    # Ù„Ø§ Ù†Ù‚ÙˆÙ… Ø¨Ø§Ù„Ø­Ø°Ù ÙÙˆØ±Ø§Ù‹ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ­ØªØ§Ø¬ Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆÙ„ÙƒÙ† Streamlit ÙŠØ¹ÙŠØ¯ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø¹Ù†Ø¯ Ø§Ù„ØªÙØ§Ø¹Ù„.
    # Ø§Ù„Ø­Ù„ Ø§Ù„Ø£Ù…Ø«Ù„ Ù‡Ù†Ø§: ØªØ±Ùƒ Ø§Ù„Ù…Ù„ÙØ§Øª Ø­ØªÙ‰ ÙŠØªÙ… Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù…ÙŠÙ„ØŒ Ø£Ùˆ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø£Ù† Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠÙ…Ø³Ø­ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©.
    # ÙÙŠ Streamlit Ø§Ù„Ø¨Ø³ÙŠØ·ØŒ Ø³Ù†ØªØ±Ùƒ Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ù„Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø¬Ù„Ø³Ø© Ø£Ùˆ Ø¨Ø¯Ø§ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø©ØŒ 
    # ÙˆÙ„ÙƒÙ† Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø³Ù†Ù‚ÙˆÙ… Ø¨ØªÙ†Ø¸ÙŠÙ Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ø¬Ù„Ø³Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠØ§Ù‹)
    # Ù‡Ù†Ø§ Ø³Ù†ÙƒØªÙÙŠ Ø¨Ø¹Ø¯Ù… Ø§Ù„Ø­Ø°Ù Ø§Ù„ÙÙˆØ±ÙŠ Ù„Ù„Ù…Ø¬Ù„Ø¯ Ù„Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„ØªØ­Ù…ÙŠÙ„.
    
    # (Ù…Ù„Ø§Ø­Ø¸Ø©: Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ù†Ø¸Ø§ÙØ©ØŒ ÙŠÙ…ÙƒÙ† Ø¬Ø¯ÙˆÙ„Ø© Ø­Ø°Ù Ø§Ù„Ù…Ø¬Ù„Ø¯ Ù„Ø§Ø­Ù‚Ø§Ù‹ØŒ Ù„ÙƒÙ† Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø­Ø§Ù„ÙŠ ÙŠØ¹Ø²Ù„ ÙƒÙ„ Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ù€ ID)

# === ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ===
st.title("ğŸ¬ Ø§Ù„Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø´Ø§Ù…Ù„ Ø§Ù„Ø³Ø±ÙŠØ¹ (60FPS Turbo)")
st.markdown("### Ø£Ù„ØµÙ‚ Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù‚Ø§Ù„ Ø£Ø¯Ù†Ø§Ù‡")

url_input_user = st.text_input("URL:", placeholder="https://www.bbc.com/news/...")

if st.button("ğŸš€ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ"):
    process_pipeline(url_input_user)