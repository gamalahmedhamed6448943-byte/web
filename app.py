import streamlit as st
import os
import sys
import subprocess
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from gtts import gTTS
from moviepy.editor import *
from moviepy.config import change_settings
from moviepy.video.fx.all import crop, resize, fadein
from PIL import Image, ImageFilter
import numpy as np
import textwrap
from rake_nltk import Rake
import nltk
import PIL.Image
import uuid
import random

# Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ ÙŠØ¹ÙŠØ¯ ØªØ¹Ø±ÙŠÙ ANTIALIAS Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙÙ‚ÙˆØ¯Ø© Ù„ÙŠØ¹Ù…Ù„ MoviePy
if not hasattr(PIL.Image, 'ANTIALIAS'):
    PIL.Image.ANTIALIAS = PIL.Image.LANCZOS

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØªØ«Ø¨ÙŠØª NLTK ---
try:
    if os.name == 'posix':
        change_settings({"IMAGEMAGICK_BINARY": "/usr/bin/convert"})
except:
    pass

@st.cache_resource
def download_nltk_resources():
    nltk.download('stopwords', quiet=True)
    nltk.download('punkt', quiet=True)
    nltk.download('punkt_tab', quiet=True)

download_nltk_resources()

# ==============================================================================
# ÙˆØ¸Ø§Ø¦Ù Ù…Ø³Ø§Ø¹Ø¯Ø© Ø¬Ø¯ÙŠØ¯Ø© (Ù„Ù„ØµÙˆØª Ø§Ù„Ø·ÙˆÙŠÙ„ ÙˆØ§Ù„Ø§Ù†ØªÙ‚Ø§Ù„Ø§Øª)
# ==============================================================================

def generate_long_audio(text, lang='en', output_file='audio.mp3'):
    """Ø¯Ø§Ù„Ø© Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø§Ù„Ø·ÙˆÙŠÙ„ ÙˆØªÙˆÙ„ÙŠØ¯ ØµÙˆØª ÙƒØ§Ù…Ù„ Ø¯ÙˆÙ† Ù‚Øµ."""
    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø¬Ù…Ù„ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ù‚Ø·Ø¹ ÙÙŠ Ù…Ù†ØªØµÙ Ø§Ù„ÙƒÙ„Ù…Ø©
    sentences = text.replace('\n', ' ').split('. ')
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        if len(current_chunk) + len(sentence) < 3000: # Ø­Ø¯ Ø¢Ù…Ù† Ù„Ù€ gTTS
            current_chunk += sentence + ". "
        else:
            chunks.append(current_chunk)
            current_chunk = sentence + ". "
    if current_chunk:
        chunks.append(current_chunk)
        
    # ØªÙˆÙ„ÙŠØ¯ Ù…Ù„ÙØ§Øª ØµÙˆØªÙŠØ© Ù„ÙƒÙ„ Ø¬Ø²Ø¡
    temp_files = []
    unique_id = uuid.uuid4().hex
    
    try:
        combined_clips = []
        for i, chunk in enumerate(chunks):
            if not chunk.strip(): continue
            chunk_filename = f"temp_chunk_{unique_id}_{i}.mp3"
            tts = gTTS(text=chunk, lang=lang)
            tts.save(chunk_filename)
            temp_files.append(chunk_filename)
            combined_clips.append(AudioFileClip(chunk_filename))
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ©
        if combined_clips:
            final_audio = concatenate_audioclips(combined_clips)
            final_audio.write_audiofile(output_file)
            final_audio.close() # Ø¥ØºÙ„Ø§Ù‚ Ù„ØªØ­Ø±ÙŠØ± Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
            return True
    except Exception as e:
        st.error(f"Error generating audio: {e}")
        return False
    finally:
        # ØªÙ†Ø¸ÙŠÙ Ù…Ù„ÙØ§Øª Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„Ù…Ø¤Ù‚ØªØ©
        for f in temp_files:
            if os.path.exists(f):
                try: os.remove(f)
                except: pass
    return False

# ==============================================================================
# 1. ÙƒÙˆØ¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±
# ==============================================================================

def get_best_image_url(img_tag, base_url):
    srcset = img_tag.get('srcset') or img_tag.get('data-srcset')
    if srcset:
        try:
            candidates = []
            for entry in srcset.split(','):
                parts = entry.strip().split()
                if len(parts) >= 1:
                    url = parts[0]
                    width = 0
                    if len(parts) > 1 and 'w' in parts[1]:
                        width = int(parts[1].replace('w', ''))
                    candidates.append((width, url))
            if candidates:
                best_candidate = sorted(candidates, key=lambda x: x[0], reverse=True)[0]
                return urljoin(base_url, best_candidate[1])
        except:
            pass
    data_src = img_tag.get('data-src') or img_tag.get('data-original')
    if data_src:
        return urljoin(base_url, data_src)
    src = img_tag.get('src')
    if src:
        return urljoin(base_url, src)
    return None

def check_image_size_is_valid(url):
    try:
        response = requests.head(url, timeout=5, allow_redirects=True)
        if response.status_code != 200:
            response = requests.get(url, stream=True, timeout=5)
        content_length = response.headers.get('Content-Length')
        if content_length:
            size_kb = int(content_length) / 1024
            if size_kb < 6:
                 return False
        return True
    except:
        return False

def advanced_extract_images(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'}
    extracted_images = []
    try:
        st.info(f"ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ± Ù…Ù†: {url} ...")
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        for tag in soup(['script', 'style', 'header', 'footer', 'nav', 'aside', 'noscript', 'iframe', 'svg']):
            tag.decompose()
        target_area = soup.find('article') or soup.find('main') or soup.find(role='main') or soup.find(id=lambda x: x and 'content' in x)
        if not target_area: target_area = soup
        img_tags = target_area.find_all('img')
        seen_urls = set()

        for i, img in enumerate(img_tags):
            full_url = get_best_image_url(img, url)
            if not full_url: continue
            full_url = full_url.split('?')[0]
            ext_check = full_url.lower()
            if ext_check.endswith('.svg') or ext_check.endswith('.gif') or ext_check.endswith('.ico'): continue
            if 'data:image' in ext_check and len(ext_check) < 1000: continue
            bad_words = ['logo', 'icon', 'avatar', 'profile', 'sprite', 'pixel', 'blank', 'transparent']
            if any(w in ext_check for w in bad_words): continue
            if full_url in seen_urls: continue
            if check_image_size_is_valid(full_url):
                extracted_images.append(full_url)
                seen_urls.add(full_url)

        if not extracted_images:
            st.warning("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØµÙˆØ± Ù…Ø­ØªÙˆÙ‰ Ø­Ù‚ÙŠÙ‚ÙŠØ©.")
        else:
            st.success(f"ğŸ‰ ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(extracted_images)} ØµÙˆØ±Ø©.")
        return extracted_images
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØµÙˆØ±: {e}")
        return []

# ==============================================================================
# 2. ÙƒÙˆØ¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ
# ==============================================================================

def extract_text_content_data(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'}
    try:
        st.info(f"ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ù†Øµ Ù…Ù†: {url} ...")
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        article_title = "No Title Found"
        h1 = soup.find('h1')
        if h1:
            article_title = h1.get_text(strip=True)
        else:
            title_tag = soup.find('title')
            if title_tag:
                article_title = title_tag.get_text(strip=True).split('-')[0].strip()

        useless_tags = ['script', 'style', 'header', 'footer', 'nav', 'aside', 'noscript', 'iframe', 'svg', 'form', 'button', 'figcaption', 'figure', 'video']
        for tag in list(soup(useless_tags)): tag.decompose()
        
        bad_classes = ['ad', 'advert', 'social', 'share', 'cookie', 'popup', 'promo', 'related-content', 'outbrain', 'taboola']
        for tag in list(soup.find_all(True)):
            if tag is None: continue
            try:
                classes = tag.get('class', [])
                if classes:
                    class_str = " ".join(classes).lower()
                    if any(bad in class_str for bad in bad_classes):
                        tag.decompose()
            except: pass

        target_area = soup.find('article') or soup.find('div', class_=lambda x: x and 'article' in x.lower() and 'body' in x.lower()) or soup.find('div', class_=lambda x: x and 'content' in x.lower()) or soup.find('main')
        if not target_area: target_area = soup
        
        paragraphs = []
        raw_text_list = []
        elements = target_area.find_all(['p', 'h2', 'h3'])
        
        for element in elements:
            text = element.get_text(strip=True)
            if len(text) < 20 and element.name == 'p': continue
            forbidden_phrases = ["Read more", "Follow us", "Copyright", "All rights reserved", "Image source", "Sign up", "Click here", "Ad Feedback", "Story highlights", "CNN", "BBC"]
            if any(phrase.lower() in text.lower() for phrase in forbidden_phrases): continue
            
            raw_text_list.append(text)
            if element.name in ['h2', 'h3']:
                paragraphs.append(f"<h3>{text}</h3>")
            else:
                paragraphs.append(f"<p>{text}</p>")

        full_clean_text = ". ".join(raw_text_list)
        st.success(f"ğŸ‰ ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(paragraphs)} ÙÙ‚Ø±Ø© Ù†ØµÙŠØ©.")
        return article_title, full_clean_text, raw_text_list
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ: {e}")
        return None, None, None

# ==============================================================================
# 3. Ù…Ø­Ø±Ùƒ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©
# ==============================================================================

def create_moving_backdrop_clip(img_path, duration, screen_size=(1280, 720)):
    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØµÙˆØ±Ø©
    pil_img = Image.open(img_path)
    
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø®Ù„ÙÙŠØ©: ØªØºÙŠÙŠØ± Ø§Ù„Ø­Ø¬Ù… Ù…Ø¹ ÙÙ„ØªØ± Ø¶Ø¨Ø§Ø¨ÙŠ
    bg_img = pil_img.resize((screen_size[0], screen_size[1]), Image.LANCZOS)
    bg_img = bg_img.filter(ImageFilter.GaussianBlur(radius=15))
    
    # Ø¬Ø¹Ù„ Ø§Ù„Ø®Ù„ÙÙŠØ© ØªØªØ­Ø±Ùƒ (Zoom) Ø¨Ø³Ø±Ø¹Ø© (0.1 Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† 0.02)
    bg_clip = ImageClip(np.array(bg_img)).set_duration(duration)
    bg_clip = bg_clip.resize(lambda t: 1 + 0.15 * t)  # Ø­Ø±ÙƒØ© Ø³Ø±ÙŠØ¹Ø© Ù…Ø³ØªÙ…Ø±Ø©
    bg_clip = bg_clip.set_position(('center', 'center'))
    
    # ØªØ­Ø¶ÙŠØ± Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©
    w, h = pil_img.size
    target_h = int(screen_size[1] * 0.9)
    ratio = w / h
    target_w = int(target_h * ratio)
    
    if target_w > screen_size[0] * 0.9:
        target_w = int(screen_size[0] * 0.9)
        target_h = int(target_w / ratio)
        
    fg_img = pil_img.resize((target_w, target_h), Image.LANCZOS)
    fg_clip = ImageClip(np.array(fg_img)).set_duration(duration)
    
    # Ø¥Ø¶Ø§ÙØ© Ø­Ø±ÙƒØ© Ø®ÙÙŠÙØ© Ù„Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ© Ø£ÙŠØ¶Ø§Ù‹ Ù„ØªØªÙ†Ø§ØºÙ… Ù…Ø¹ Ø§Ù„Ø®Ù„ÙÙŠØ©
    fg_clip = fg_clip.resize(lambda t: 1 + 0.05 * t)
    fg_clip = fg_clip.set_position(('center', 'center'))
    
    final_clip = CompositeVideoClip([bg_clip, fg_clip], size=screen_size).set_duration(duration)
    return final_clip

def generate_youtube_metadata(title, text_list, url):
    full_text = " ".join(text_list)
    r = Rake()
    r.extract_keywords_from_text(full_text)
    keywords = r.get_ranked_phrases()[:15]
    tags = [k for k in keywords if len(k) < 30]
    tags_str = ", ".join(tags)
    
    summary = "\n\n".join(text_list[:3])
    description = f""" {title} \n\n {summary} \n\n ğŸ‘‡ Read the full article here: {url} \n\n #News #{tags[0].replace(' ','')} #{tags[1].replace(' ','') if len(tags)>1 else 'Video'} """.strip()
    
    thumb_prompt = f"A high-quality YouTube thumbnail image representing '{title}'. Professional news style, high contrast, 4k resolution, featuring elements of {tags[0] if tags else 'news'}."
    
    return tags_str, description, thumb_prompt

def process_pipeline(url_input):
    if not url_input:
        st.warning("âŒ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø±Ø§Ø¨Ø·.")
        return

    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø±Ù ÙØ±ÙŠØ¯ Ù„Ù„Ø¬Ù„Ø³Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ù„Ø¹Ø¯Ù… Ø®Ù„Ø· Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
    session_id = uuid.uuid4().hex
    
    # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
    title, full_text, text_list = extract_text_content_data(url_input)
    if not title or not full_text:
        st.error("âŒ ÙØ´Ù„ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ.")
        return
        
    images_urls = advanced_extract_images(url_input)
    if not images_urls:
        st.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØµÙˆØ±ØŒ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø´Ø§Ø´Ø© Ø³ÙˆØ¯Ø§Ø¡ Ù…Ø¹ Ø§Ù„Ù†Øµ.")

    # 2. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØª (TTS)
    audio_file = f"generated_audio_{session_id}.mp3"
    with st.spinner("ğŸ”Š Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØª..."):
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ (Ø¹Ù†ÙˆØ§Ù† + Ù…Ø­ØªÙˆÙ‰) Ø¨Ø¯ÙˆÙ† Ù‚Øµ
        tts_text = f"{title}. {full_text}"
        
        # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„ØªÙ‚Ø³ÙŠÙ… ÙˆØ¯Ù…Ø¬ Ø§Ù„ØµÙˆØª
        if not generate_long_audio(tts_text, 'en', audio_file):
             # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù‚ØµÙŠØ±Ø© Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„Ø·ÙˆÙŠÙ„
             tts = gTTS(text=tts_text[:1000], lang='en')
             tts.save(audio_file)
        
        audio_clip = AudioFileClip(audio_file)
        audio_duration = audio_clip.duration
        st.success(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØª. Ø§Ù„Ù…Ø¯Ø©: {audio_duration:.2f} Ø«Ø§Ù†ÙŠØ©")

    # 3. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
    output_filename = f"output_video_{session_id}.mp4"
    
    with st.spinner("ğŸ¬ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± ÙˆØ¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ..."):
        if images_urls:
            downloaded_images = []
            for i, img_url in enumerate(images_urls):
                try:
                    img_data = requests.get(img_url).content
                    img_name = f"temp_img_{session_id}_{i}.jpg"
                    with open(img_name, 'wb') as handler:
                        handler.write(img_data)
                    downloaded_images.append(img_name)
                except:
                    continue
            
            if not downloaded_images:
                st.error("âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±.")
                return

            img_duration = audio_duration / len(downloaded_images)
            clips = []
            
            # Ø§Ø®ØªÙŠØ§Ø± Ù†ÙˆØ¹ Ø§Ù†ØªÙ‚Ø§Ù„ Ø¹Ø´ÙˆØ§Ø¦ÙŠ (Transition) Ù„Ù‡Ø°Ø§ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„
            # Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª: crossfadein (ØªÙ„Ø§Ø´ÙŠ), fadein (Ø¸Ù‡ÙˆØ± Ù…Ù† Ø£Ø³ÙˆØ¯), None (Ù‚Øµ Ù…Ø¨Ø§Ø´Ø±)
            transition_style = random.choice(['crossfade', 'fadein_black', 'sharp'])
            st.info(f"âœ¨ ØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ù†Ù…Ø· Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„: {transition_style}")

            for img_path in downloaded_images:
                clip = create_moving_backdrop_clip(img_path, img_duration)
                
                # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ø§Ù„Ù…Ø®ØªØ§Ø±
                if transition_style == 'crossfade':
                    clip = clip.crossfadein(1.0)
                elif transition_style == 'fadein_black':
                    clip = clip.fadein(1.0)
                # Ø¥Ø°Ø§ ÙƒØ§Ù† sharp Ù„Ø§ Ù†Ø¶ÙŠÙ ØªØ£Ø«ÙŠØ±
                
                clips.append(clip)
            
            # Ø¯Ù…Ø¬ Ø§Ù„ÙƒÙ„ÙŠØ¨Ø§Øª
            # padding=-1 Ø¶Ø±ÙˆØ±ÙŠ Ù„Ø¹Ù…Ù„ crossfade Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­
            pad_val = -1 if transition_style == 'crossfade' else 0
            final_video = concatenate_videoclips(clips, method="compose", padding=pad_val)
            
            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ø¯Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù„Ù„ØµÙˆØª (Ù‚Ø¯ ØªØ®ØªÙ„Ù Ù‚Ù„ÙŠÙ„Ø§Ù‹ Ø¨Ø³Ø¨Ø¨ Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„Ø§Øª)
            if final_video.duration < audio_duration:
                # ØªÙ…Ø¯ÙŠØ¯ Ø¢Ø®Ø± Ø¥Ø·Ø§Ø± Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
                pass 
        else:
            color_clip = ColorClip(size=(1280, 720), color=(0,0,0), duration=audio_duration)
            txt_clip = TextClip(title, fontsize=70, color='white', size=(1000, None), method='caption')
            txt_clip = txt_clip.set_position('center').set_duration(audio_duration)
            final_video = CompositeVideoClip([color_clip, txt_clip])

        # 4. Ø¯Ù…Ø¬ Ø§Ù„ØµÙˆØª ÙˆØªØµØ¯ÙŠØ± Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
        final_video = final_video.set_audio(audio_clip)
        
        st.text("âš™ï¸ Ø¬Ø§Ø±ÙŠ ØªØµØ¯ÙŠØ± Ø§Ù„ÙÙŠØ¯ÙŠÙˆ (Rendering)... FPS = 1")
        # ØªÙ… Ø¶Ø¨Ø· FPS Ø¹Ù„Ù‰ 1 ÙƒÙ…Ø§ Ø·ÙÙ„Ø¨
        final_video.write_videofile(output_filename, fps=1, codec="libx264", audio_codec="aac")

        # 5. ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª (Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ø±Ù Ø§Ù„ÙØ±ÙŠØ¯)
        if images_urls:
            for f in downloaded_images:
                try: os.remove(f)
                except: pass
        try: os.remove(audio_file)
        except: pass

        # 6. Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
        st.success("âœ… COMPLETED SUCCESSFULLY")
        
        tags, desc, thumb = generate_youtube_metadata(title, text_list, url_input)
        
        st.subheader("ğŸ“‹ YOUTUBE DATA")
        st.text_area("Title", title)
        st.text_area("Description", desc)
        st.text_area("Tags", tags)
        st.info(f"**Thumbnail Prompt:** {thumb}")
        
        st.subheader("ğŸ¥ FINAL VIDEO")
        st.video(output_filename)
        
        with open(output_filename, "rb") as file:
            st.download_button(
                label="ğŸ“ Download Video",
                data=file,
                file_name=f"generated_video_{session_id}.mp4",
                mime="video/mp4"
            )
            
        # Ø­Ø°Ù Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ù…Ø³Ø§Ø­Ø©)
        # try: os.remove(output_filename)
        # except: pass

# === ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ===
st.title("ğŸ¬ Ø§Ù„Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø´Ø§Ù…Ù„: Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ø¥Ù„Ù‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ")
st.markdown("### Ø£Ù„ØµÙ‚ Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù‚Ø§Ù„ Ø£Ø¯Ù†Ø§Ù‡")

url_input_user = st.text_input("URL:", placeholder="https://www.bbc.com/news/...")

if st.button("ğŸš€ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ"):
    process_pipeline(url_input_user)